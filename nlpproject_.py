# -*- coding: utf-8 -*-
"""NLPproject_ARU2330_9003.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D45VJQenqX2gCWl1ujoPImJNHSkimvUO
"""

pip install datasets

"""To download the English SQuAD data"""

from datasets import load_dataset

data = load_dataset('squad')

data['train']['answers'][:5]

"""**Formatting Answers**

Before we begin tokenization, training, etc — we need to reformat our answers feature into the correct format for training.
"""

from tqdm.auto import tqdm  # for showing progress bar

def add_end_idx(answers, contexts):
    new_answers = []
    # loop through each answer-context pair
    for answer, context in tqdm(zip(answers, contexts)):
        # quick reformating to remove lists
        answer['text'] = answer['text'][0]
        answer['answer_start'] = answer['answer_start'][0]
        # gold_text refers to the answer we are expecting to find in context
        gold_text = answer['text']
        # we already know the start index
        start_idx = answer['answer_start']
        # and ideally this would be the end index...
        end_idx = start_idx + len(gold_text)

        # ...however, sometimes squad answers are off by a character or two
        if context[start_idx:end_idx] == gold_text:
            # if the answer is not off :)
            answer['answer_end'] = end_idx
        else:
            # this means the answer is off by 1-2 tokens
            for n in [1, 2]:
                if context[start_idx-n:end_idx-n] == gold_text:
                    answer['answer_start'] = start_idx - n
                    answer['answer_end'] = end_idx - n
        new_answers.append(answer)
    return new_answers

def prep_data(dataset):
    questions = dataset['question']
    contexts = dataset['context']
    answers = add_end_idx(
        dataset['answers'],
        contexts
    )
    return {
        'question': questions,
        'context': contexts,
        'answers': answers
    }

dataset = prep_data(data['train'])

dataset['answers'][:5]

"""**Tokenization**
We need to tokenize the SQuAD data so that it is readable by our Bert model. For the context and question features we can do using the standard tokenizer() function:
"""

from transformers import BertTokenizerFast

tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')
# tokenize
train = tokenizer(dataset['context'], dataset['question'],
                  truncation=True, padding='max_length',
                  max_length=512, return_tensors='pt')

"""We now how context-question pairs represented as Encoding objects. Let's take a look at a decoded context-question pair."""

tokenizer.decode(train['input_ids'][0])[:855]

"""Encoding both our context and question strings into single arrays of tokens. This will act as the input to our Q&A training, but we have no targets yet.

Our targets are the start and end positions of the answer, which we previously built using the character start and end positions within the context strings. However, we will be feeding tokens into Bert, so we need to provide the token start and end positions.

To do this, we need to convert the character start and end positions into token start and end positions — easily done with our add_token_positions function:
"""

def add_token_positions(encodings, answers):
    # initialize lists to contain the token indices of answer start/end
    start_positions = []
    end_positions = []
    for i in tqdm(range(len(answers))):
        # append start/end token position using char_to_token method
        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))
        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end']))

        # if start position is None, the answer passage has been truncated
        if start_positions[-1] is None:
            start_positions[-1] = tokenizer.model_max_length
        # end position cannot be found, char_to_token found space, so shift position until found
        shift = 1
        while end_positions[-1] is None:
            end_positions[-1] = encodings.char_to_token(i, answers[i]['answer_end'] - shift)
            shift += 1
    # update our encodings object with the new token-based start/end positions
    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})

# apply function to our data
add_token_positions(train, dataset['answers'])

train.keys()

train['start_positions'][:5], train['end_positions'][:5]

"""**Training**

We will be training using PyTorch, which means we will need to convert the tensors we’ve built into a PyTorch Dataset object.
"""

import torch

class SquadDataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __getitem__(self, idx):
        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}

    def __len__(self):
        return len(self.encodings.input_ids)

# build datasets for both our training data
train_dataset = SquadDataset(train)

"""We will feed our Dataset to our Q&A training loop using a Dataloader object, which we initialize with:"""

loader = torch.utils.data.DataLoader(train_dataset,
                                     batch_size=16,
                                     shuffle=True)

from transformers import BertForQuestionAnswering, BertConfig

# Define BERT configuration
config = BertConfig(
    hidden_size=768,
    num_attention_heads=12,
    num_hidden_layers=12,
    intermediate_size=3072,
    hidden_dropout_prob=0.1,
    attention_probs_dropout_prob=0.1,
)

model = BertForQuestionAnswering(config)

"""And finally, we setup our model parameters and begin the training loop."""

from transformers import AdamW

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model.to(device)
model.train()
optim = AdamW(model.parameters(), lr=5e-5)

"""we use only 1 epoch to let it be done in approx. 3 hours  time. the epochs can be increased accordingly."""

for epoch in range(1):
    loop = tqdm(loader)
    for batch in loop:
        optim.zero_grad()

        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        start_positions = batch['start_positions'].to(device)
        end_positions = batch['end_positions'].to(device)

        outputs = model(input_ids, attention_mask=attention_mask,
                        start_positions=start_positions,
                        end_positions=end_positions)

        loss = outputs[0]
        loss.backward()
        optim.step()

        loop.set_description(f'Epoch {epoch}')
        loop.set_postfix(loss=loss.item())

"""Finally, saving our model"""

model.save_pretrained('/content/ARUdrive/bert-qa')